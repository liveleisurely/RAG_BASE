
# ==========================================
# ê°„ë‹¨í•œ Mock RAG
# ==========================================


import asyncio
import json
import uvicorn
import operator
import logging
from typing import TypedDict, List, Annotated
from fastapi import FastAPI
from fastapi.responses import StreamingResponse, JSONResponse, FileResponse
from pydantic import BaseModel, Field

# LangGraph ê´€ë ¨ ì„í¬íŠ¸
from langgraph.graph import StateGraph, END

# ==========================================
# [0] ë¡œê¹…(Logging) ì„¤ì • (Fix: handlers ì˜¤íƒ€ ìˆ˜ì •)
# ==========================================
# print() ëŒ€ì‹  ìš´ì˜ í™˜ê²½ì— ë‚¨ê¸¸ ë¡œê·¸ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
logger = logging.getLogger("RAG_BASE")
logger.setLevel(logging.INFO)

# í•¸ë“¤ëŸ¬ ì„¤ì • (ì½˜ì†” ì¶œë ¥)
console_handler = logging.StreamHandler()
formatter = logging.Formatter("%(asctime)s [%(levelname)s] %(message)s")
console_handler.setFormatter(formatter)
logger.addHandler(console_handler)

# ==========================================
# [1] ë°ì´í„° ëª¨ë¸ ì •ì˜ (Schema & State)
# ==========================================
class ChatRequest(BaseModel):
    question: str = Field(..., description="ì‚¬ìš©ì ì§ˆë¬¸", example="LangGraph ë³‘ë ¬ ì²˜ë¦¬")
    stream: bool = Field(False, description="ìŠ¤íŠ¸ë¦¬ë° ì—¬ë¶€", example=True)

class GraphState(TypedDict):
    question: str
    rewritten_query: str
    # ğŸ’¡ í•µì‹¬: ë³‘ë ¬ ë…¸ë“œ ê²°ê³¼ê°€ ë®ì–´ì”Œì›Œì§€ì§€ ì•Šê³  í•©ì³ì§€ë„ë¡(Add) ì„¤ì •
    context: Annotated[List[str], operator.add]
    answer: str

# ==========================================
# [2] Mock ë¡œì§ (ì‹¤ì œ ì—°ë™ ì‹œ LLM/DB ì½”ë“œë¡œ êµì²´)
# ==========================================
async def mock_rewrite_logic(question: str) -> str:
    # logger.info(f"ì¿¼ë¦¬ ìµœì í™” ë¡œì§ ìˆ˜í–‰ ì¤‘: {question}")
    await asyncio.sleep(0.5)
    return f"ìµœì í™”ëœ ì¿¼ë¦¬: {question}"

async def mock_search_source_a(query: str) -> List[str]:
    await asyncio.sleep(1.0)
    return [f"[Wiki] '{query}' ê²€ìƒ‰ ê²°ê³¼"]

async def mock_search_source_b(query: str) -> List[str]:
    await asyncio.sleep(1.0)
    return [f"[CorpDB] '{query}' ì‚¬ë‚´ ë¬¸ì„œ"]

async def mock_answer_generator(context: List[str]):
    full_context = " | ".join(context)
    response_text = f"ìˆ˜ì§‘ëœ {len(context)}ê°œì˜ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.\n(ì¶œì²˜: {full_context})"
    
    # í† í° ë‹¨ìœ„ ìŠ¤íŠ¸ë¦¬ë° ì‹œë®¬ë ˆì´ì…˜
    for char in response_text:
        await asyncio.sleep(0.05)
        yield char

# ==========================================
# [3] LangGraph ë…¸ë“œ ì •ì˜
# ==========================================
async def rewrite_node(state: GraphState):
    logger.info(f"ğŸ”„ [Rewrite] ì‹œì‘ - ì§ˆë¬¸: {state['question']}")
    new_query = await mock_rewrite_logic(state['question'])
    return {"rewritten_query": new_query}

async def search_node_1(state: GraphState):
    logger.info(f"ğŸ” [Search1] ìœ„í‚¤ ê²€ìƒ‰ ì‹œì‘")
    docs = await mock_search_source_a(state['rewritten_query'])
    return {"context": docs}

async def search_node_2(state: GraphState):
    logger.info(f"ğŸ” [Search2] ì‚¬ë‚´ DB ê²€ìƒ‰ ì‹œì‘")
    docs = await mock_search_source_b(state['rewritten_query'])
    return {"context": docs}

async def generate_node(state: GraphState):
    logger.info(f"ğŸ§  [Generate] ë‹µë³€ ìƒì„± ì‹œì‘ (Context: {len(state['context'])}ê°œ)")
    # Non-stream ìš”ì²­ì„ ìœ„í•´ ì „ì²´ í…ìŠ¤íŠ¸ ìƒì„±
    chunks = [c async for c in mock_answer_generator(state['context'])]
    return {"answer": "".join(chunks)}

# ==========================================
# [4] ê·¸ë˜í”„ ì¡°ë¦½ (Topology)
# ==========================================
workflow = StateGraph(GraphState)

# ë…¸ë“œ ë“±ë¡
workflow.add_node("rewrite", rewrite_node)
workflow.add_node("search1", search_node_1)
workflow.add_node("search2", search_node_2)
workflow.add_node("generate", generate_node)

# ì—£ì§€ ì—°ê²° (ë³‘ë ¬ ì²˜ë¦¬ êµ¬ì¡°)
workflow.set_entry_point("rewrite")
workflow.add_edge("rewrite", "search1") # Fan-out
workflow.add_edge("rewrite", "search2") # Fan-out
workflow.add_edge("search1", "generate") # Fan-in
workflow.add_edge("search2", "generate") # Fan-in
workflow.add_edge("generate", END)

app_graph = workflow.compile()

# ==========================================
# [5] FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜
# ==========================================
app = FastAPI(title="RAG_BASE Service")

@app.get("/")
async def root():
    return {"message": "RAG_BASE Server Running. Visit /docs"}

@app.get("/favicon.ico", include_in_schema=False)
async def favicon():
    return FileResponse("favicon.ico")

@app.post("/chat")
async def chat_endpoint(req: ChatRequest):
    # ì´ˆê¸° ìƒíƒœ (ContextëŠ” ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì‹œì‘)
    initial_state = {"question": req.question, "context": []}

    # CASE A: ìŠ¤íŠ¸ë¦¬ë° (SSE)
    if req.stream:
        async def event_generator():
            # [íŠ¸ë™ 1] LangGraph ì´ë²¤íŠ¸ ê°ì§€ (ì„œë²„ ë¡œê¹… + ìƒíƒœ ë©”ì‹œì§€ ì „ì†¡)
            async for event in app_graph.astream_events(initial_state, version="v2"):
                kind = event["event"]
                name = event["name"]
                meta = event.get("metadata", {})
                node_id = meta.get("langgraph_node")

                # âœ¨ ê±°ìš¸ ê¸°ë²•: ì§„ì§œ ë…¸ë“œì˜ ì‹œì‘ë§Œ í•„í„°ë§
                if kind == "on_chain_start" and node_id and name == node_id:
                    # í”„ë¡ íŠ¸ì—”ë“œìš© ìƒíƒœ ë©”ì‹œì§€ ìƒì„±
                    status_msg = ""
                    if node_id == "rewrite": status_msg = "âœï¸ ì§ˆë¬¸ì„ ìµœì í™”í•˜ê³  ìˆìŠµë‹ˆë‹¤..."
                    elif "search" in node_id: status_msg = f"ğŸ” {node_id}ì—ì„œ ìë£Œ ê²€ìƒ‰ ì¤‘..."
                    elif node_id == "generate": status_msg = "ğŸ¤– ë‹µë³€ì„ ì‘ì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."
                    
                    if status_msg:
                        payload = json.dumps({'type': 'status', 'content': status_msg}, ensure_ascii=False)
                        yield f"data: {payload}\n\n"

            # [íŠ¸ë™ 2] ì‹¤ì œ ë‹µë³€ ìŠ¤íŠ¸ë¦¬ë° (Mock)
            # (ì‹¤ì œ êµ¬í˜„ ì‹œì—” ìœ„ astream_events ë£¨í”„ ë‚´ on_chat_model_stream ì‚¬ìš©)
            dummy_context = ["Mock Data"]
            async for char in mock_answer_generator(dummy_context):
                payload = json.dumps({'type': 'token', 'content': char}, ensure_ascii=False)
                yield f"data: {payload}\n\n"

            yield "data: [DONE]\n\n"

        return StreamingResponse(
            event_generator(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no"
            }
        )

    # CASE B: ì¼ë°˜ ìš”ì²­ (JSON)
    else:
        result = await app_graph.ainvoke(initial_state)
        return JSONResponse(content={
            "question": result["question"],
            "answer": result["answer"],
            "sources": result["context"]
        })

if __name__ == "__main__":
    print("ğŸš€ [RAG_BASE] ì„œë²„ ì‹œì‘: http://0.0.0.0:8000")
    uvicorn.run(app, host="0.0.0.0", port=8000)
